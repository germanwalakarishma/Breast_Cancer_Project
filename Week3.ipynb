{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyX7-uEIOvqg",
        "colab_type": "text"
      },
      "source": [
        "**Distributions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_lZHFbpC5ml",
        "colab_type": "text"
      },
      "source": [
        "Disturbution of dataset  shows the frequency at the which possible values occur within an interval.\n",
        "\n",
        "**Normal distribution/Gaussian ditributions** is represented by bell curve since high distribution is found at the middle and decreases towards its tail.\n",
        "\n",
        "There will be more concentration around the mean and will be symmetrical to both the sides of the mean.\n",
        "\n",
        "If mean is smaller or bigger, distribution of the graph remains same but the graph will be shifted to the left side of the plane and right side of the plane, respectively.(keeping standard devaition constant)\n",
        "\n",
        "Also how much the distribution is spreaded is determined by standard deviation.\n",
        "\n",
        "If mean is constant and standard deviation is lower, graph remains same but there will be more data in the middle and less towards its tails.\n",
        "\n",
        "If standard deviation is higher, less data in the middle while more towards its tails.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Binominal Distributions** allows us to compute the probability when the number of observations is constant and possible outcome is either success or failure on each trials or when repeated multiple times.\n",
        "\n",
        "\n",
        "\n",
        "Normal distribution is continous distribution while binomial is discrete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO1YyL9tOrcF",
        "colab_type": "text"
      },
      "source": [
        "**Logistics Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWIrtFZgpZ2v",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "1. Logistics Regression is generally binary classification.\n",
        "2. It is used when the dependent variable(target) is categorical.\n",
        "\n",
        "     For example,\n",
        "      *   To predict whether an email is spam (1) or (0)\n",
        "      *   Whether the tumor is malignant (1) or not (0)\n",
        "      *   Whether a person is obese(1) or not(0)\n",
        "\n",
        "3. For linear regression:\n",
        "      *   Threshold value has to be set\n",
        "\n",
        "4. We choose the best fit line such that we are able to classify the data points easily.\n",
        "\n",
        "5. Now let's say one outlier is added to the dataset which will cause best fit line to change and due to which our classification of data doesn't work correctly and gives the possibility with high error rate.\n",
        "\n",
        "    Eg : Say if the actual class is obese, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not obese which can lead to serious consequence in real time.\n",
        "\n",
        "\n",
        "6. Everytime based on data points or dataset given, our best fit line changes and due to outliers present in the dataset our best fit line gets deviated which is major drawback of linear regression.\n",
        "\n",
        "\n",
        "7. From this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.   \n",
        "\n",
        "8. Logistics Regression is usually applied to a problem statement where classification problem can be linearly separable.\n",
        "\n",
        "9. Assumptions:\n",
        "      *   positive points are considered as +1 \n",
        "      *   negative points are considered as -1. \n",
        "      *   Distance between a particular positive point and above the slope (plane) will be always positive \n",
        "      *   Distance between a particular negative point and below the slope is always negative.\n",
        "\n",
        "10. [Sample Graph](https://drive.google.com/file/d/1HpOrIxZuzo-OSu69oKT9JRW85sh02dZE/view?usp=sharing)\n",
        "\n",
        "11. In the following equation y=mx+c or y = (w^T)*x + b where c and b are intercepts.\n",
        "\n",
        "Since our orgin is passing through 0, in that case our equation becomes y = mx or y = (w^T)x where (w^T)x can be considered as the distance between a particular point and the plane or the line drawn.\n",
        "\n",
        "Summation can be written as summation of data from i=1 to n of y(i) * (w(i)^T)*x(i)\n",
        "\n",
        "12. For eg: lets consider some cases :\n",
        "\n",
        "  *  x point(positive) lies above the plane so y = +1, and distance between a point to above the plane is positive (w^T)x > 0. so y * (w^T)x > 0\n",
        "\n",
        "  *  x point(negative) lies below the plane so y=-1,and distance between a point to below the plane is positive (w^T)x < 0. so y * (w^T)x > 0 since both the parameters are negative.\n",
        "\n",
        "  *  x point(negative) lies above the plane so y = -1, and distance between a point to above the plane is positive (w^T)x > 0. so y * (w^T)x < 0 since one parameter is negative and other one is positive.\n",
        "\n",
        "  *  x point(postive) lies below the plane so y=+1,and distance between a point to below the plane is positive (w^T)x < 0. so y * (w^T)x < 0.\n",
        "\n",
        "13. Conculsion : If y * (w^T)x < 0, then that point is incorrectly classified. Our cost function i=1 to n of y(i) * (w(i)^T)x(i) should be maximum as possible.\n",
        "\n",
        "14. If I want to create best fit line which linearly separates the points,I have to make sure that the summation of all points to the distance form the slope(plane) should be maximum.\n",
        "\n",
        "Lets say , our max value is -500 which classifies almost all data points correctly.Now lets say we have one outlier and max value is now 2, so in this case we choose value 2 as it is maximum and our best fit line changes. But due to an outlier, most of the points are not classified correctly.\n",
        "\n",
        "In some cases like above, it gives a very very high negative value,due to this reason , sigmoid function is introduced. sigmoid(y(i) * (w(i)^T)*x(i)) or sigmoid(z) = (1/1+e^-z) which in turn returns value between 0 to 1.\n",
        "\n",
        "15. The sigmoid function, also called logistic function gives an 'S' shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs0D0ooS-lzv",
        "colab_type": "text"
      },
      "source": [
        "**Decision Tree Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqPuwWq4O4zu",
        "colab_type": "text"
      },
      "source": [
        "1. It is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. \n",
        "\n",
        "2. The basic idea behind any decision tree algorithm is as follows:\n",
        "\n",
        "  * Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n",
        "  * Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
        "  * Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
        "      * All the tuples belong to the same attribute value.\n",
        "      * There are no more remaining attributes.\n",
        "      * There are no more instances.\n",
        "\n",
        "3. Attribute selection measures are Information Gain,Gain Ratio and Gini Index.\n",
        "\n",
        "4. Entropy plays an important part here as it determines the purity of the input set or subset.\n",
        "\n",
        "5. Decision trees are easy to interpret and visualize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHlaZXf_RF8K",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeeD_-OrRIHv",
        "colab_type": "text"
      },
      "source": [
        "1. Random forests is a supervised learning algorithm. It can be used both for classification and regression.\n",
        "\n",
        "2. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting.\n",
        "\n",
        "3. It also provides a pretty good indicator of the feature importance.\n",
        "\n",
        "4. It works in four steps:\n",
        "\n",
        "  * Select random samples from a given dataset.\n",
        "\n",
        "  * Construct a decision tree for each sample and get a prediction result from each decision tree.\n",
        "\n",
        "  * Perform a vote for each predicted result.\n",
        "\n",
        "  * Select the prediction result with the most votes as the final prediction.\n",
        "\n",
        "5. It is considered as a highly accurate and robust method because of the number of decision trees participating in the process.It also does not suffer from overfitting problem.\n",
        "\n",
        "6. It is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.\n",
        "\n",
        "7. Random forests is difficult to interpret."
      ]
    }
  ]
}